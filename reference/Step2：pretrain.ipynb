{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a008d160",
   "metadata": {},
   "source": [
    "# Pretrain 预训练详解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb88df1",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f00933-f00a-422e-aaa4-dc1b6a80aeff",
   "metadata": {},
   "source": [
    "- Step 1.导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f663d8f8-3a99-4f19-a0c6-a2dde386e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "import json\n",
    "import jsonlines\n",
    "import psutil\n",
    "import ujson\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8f30c-d2fd-41cb-aea1-8addb7279fdf",
   "metadata": {},
   "source": [
    "- Step 2.定义BOS和EOS标记，并加载分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff895d52-6ce9-4026-b30d-6432a79c8d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义BOS和EOS标记\n",
    "bos_token = \"<s>\"      # beginning of sentence\n",
    "eos_token = \"</s>\"     # end of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "132613a9-e31d-493b-9bed-e70f7cf38f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载的tokenizer词表大小: 6400\n"
     ]
    }
   ],
   "source": [
    "# 加载训练好的分词器路径\n",
    "tokenizer = AutoTokenizer.from_pretrained('../model/mateconv_tokenizer', use_fast=False)\n",
    "print(f'加载的tokenizer词表大小: {len(tokenizer)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eed63f5-2774-4e49-ac14-53362b7d3a41",
   "metadata": {},
   "source": [
    "- Step 3.读取部分数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba5e5a48-e767-422a-81a0-29d973c4352f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 行数据: {'text': '在查处虚开增值税专用发票案件中，常常涉及进项留抵税额和税款损失的认定和处理。在计算税款损失时，要不要将进项留抵税额包括在内？\\n对此，实务中存在意见分歧。\\n有人主张归并，即计算税款损失时包括进项留抵税额；\\n有人主张剥离，即计算税款损失时剔除进项留抵税额。分析这个问题，需要确定进项留抵税额与税款损失之间是什么关系。\\n理清这二者之间的关系，首先需要了解增值税的概念和其抵扣机制。增值税是以商品（货物、服务等）在流转过程中产生的增值额作为计税依据而征收的一种流转税。为避免重复征税，在增值税中存在抵扣链条机制。\\n一般而言，交易上游企业缴纳的税额，交易下游企业可以对相应的税额进行抵扣。\\n对增值税一般纳税人来说，其购进货物、服务等取得增值税专用发票，发票上的税额是进项税额。\\n其出售货物、服务等，向购买方开具增值税专用发票，发票的税额是销项税额。\\n一般情况下，销项税额减去进项税额的金额是应纳税额，企业根据应纳税额按期申报纳税。\\n其次需要了解进项留抵税额的概念及产生原因。\\n在计算销项税额和进项税额的差额时，有时会出现负数，即当期进项税额大于当期销项税额。这个差额在当期未实现抵扣，为进项留抵税额，在以后纳税人有销项税额时再进行抵扣。\\n企业产生进项留抵税额的主要原因是其进项税额和销项税额时间上的不一致。\\n例如，企业前期集中采购货物和服务，投资大，销项税率低于进项税率等。\\n从税款抵扣的角度看，进项留抵税额只是购进的这部分进项税额参与到增值税应纳税额的计算过程中，但是其对应的进项税额抵扣还未真正实现，一般要等到其未来有相应的销项税额时，才能真正实现进项税额抵扣。\\n可见，进项留抵税额处于不确定状态，能否抵扣受到很多因素影响，例如企业经营中断，没有销项税额，这时进项留抵税额就无法实现抵扣。但如果企业按照税收政策规定申请进项留抵退税，进项税额抵扣就随之实现。\\n最后需要了解税款损失的概念。\\n税款损失，通常是指因虚开增值税专用发票，导致国家税款被骗或者流失的金额。关于税款损失，实务中有多种表述。\\n例如，北京大学法学院教授陈兴良曾谈到虚开行为本身不会造成国家税款损失，只有利用发票抵扣时才会造成国家税款损失。刘兵等编著的《虚开增值税专用发票案例司法观点和案例解析》一书中提到：“给国家税款造成损失的数额，实际上就是被骗取的国家税款在侦查终结以前无法追回的部分。”\\n赵清海与王家欣合著的《增值税专用发票虚开的判定与预防》一书中提到：“司法实践中，受票方用虚开的增值税专用发票予以抵扣的税款，从而导致受票方应纳税额的减少是法院所认定的国家税款流失的金额。”\\n从这些表述可见，税款损失应该是实际造成的损失，不应包括不确定的部分——进项留抵税额，进项留抵税额与税款损失之间不能直接画等号。\\n综上分析，进项留抵税额，只是使国家税款处于可能被抵扣的状态，还没有真正造成国家税款流失，一般情况下应将其从税款损失中剥离，特殊条件下将其归并入税款损失。\\n例如，当纳税人造假按照税收政策规定申请进项留抵税额退税后，有关税款损失将会从危险状态转化成危害结果，这时候要将有关进项留抵税额并入税款损失。\\n所以，在虚开增值税专用发票案件中，一般情况下，如果以纳税人的进项税额作为税款损失的计算基数，在对其进行行政处罚或刑事处罚时，应把进项留抵税额从税款损失中剔除，但纳税人申请进项留抵退税的除外。这样处理，把处罚与危害结果相对应，体现行政处罚法的过罚相当原则和刑法的罚当其罪原则。'}\n"
     ]
    }
   ],
   "source": [
    "def preview_dataset(file_path, num_lines=5):\n",
    "    \"\"\"\n",
    "    读取并展示数据集的前 num_lines 行\n",
    "    \"\"\"\n",
    "    # 检查文件是否存在\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"{file_path} 文件不存在，请检查路径！\")\n",
    "\n",
    "    # 逐行读取并展示前 num_lines 行\n",
    "    with jsonlines.open(file_path) as reader:\n",
    "        for idx, obj in enumerate(reader):\n",
    "            print(f\"第 {idx + 1} 行数据: {obj}\")\n",
    "            if idx + 1 >= num_lines:\n",
    "                break\n",
    "\n",
    "# 指定文件路径和需要展示的行数\n",
    "file_path = '../dataset/mobvoi_seq_monkey_general_open_corpus/mobvoi_seq_monkey_general_open_corpus.jsonl'\n",
    "preview_dataset(file_path, num_lines=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01a655d-02e9-48f2-b7f3-164f49dfd5cd",
   "metadata": {},
   "source": [
    "- Step 4.统计与清理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "075d0ca7-57ac-4b99-8421-ee58b46226fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_lines(file_path):\n",
    "    \"\"\"\n",
    "    获取 JSONL 文件的总行数，不忽略错误，保证能够全面统计。\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:  # 使用二进制模式避免编码问题\n",
    "        return sum(1 for _ in f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca4b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../dataset/mobvoi_seq_monkey_general_open_corpus/mobvoi_seq_monkey_general_open_corpus.jsonl'\n",
    "sum = get_total_lines(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b190ec6-a982-4080-8430-2792201557d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_jsonl_format(file_path):\n",
    "    \"\"\"\n",
    "    检查 JSONL 文件中的每一行是否是有效的 JSON 格式，带进度显示，并统计所有有问题的行。\n",
    "    \"\"\"\n",
    "    total_lines = get_total_lines(file_path)  # 获取文件总行数\n",
    "    valid_lines = 0\n",
    "    invalid_lines = 0\n",
    "\n",
    "    # 使用逐行读取，捕获 JSON 和编码错误\n",
    "    with open(file_path, 'rb') as f:  # 使用二进制读取避免编码问题\n",
    "        # 使用 tqdm 进度条显示检查进度\n",
    "        for idx, line in tqdm(enumerate(f), total=total_lines, desc=\"Checking JSONL format\"):\n",
    "            try:\n",
    "                # 先尝试将每行数据解码为 UTF-8\n",
    "                decoded_line = line.decode('utf-8')\n",
    "                # 然后检查是否是有效的 JSON 格式\n",
    "                obj = jsonlines.Reader([decoded_line]).read()\n",
    "                valid_lines += 1\n",
    "            except UnicodeDecodeError as e:\n",
    "                print(f\"Encoding error at line {idx + 1}: {e}\")\n",
    "                invalid_lines += 1\n",
    "            except jsonlines.InvalidLineError as e:\n",
    "                print(f\"Invalid JSON at line {idx + 1}: {e}\")\n",
    "                invalid_lines += 1\n",
    "\n",
    "    print(f\"检查完成，文件中共有 {valid_lines} 行有效的 JSON 数据，{invalid_lines} 行无效的 JSON 数据。\")\n",
    "    return valid_lines, invalid_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2dd08cb-05f9-4cb6-9075-2e723f98fa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking JSONL format: 100%|██████████| 13000000/13000000 [02:56<00:00, 73739.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检查完成，文件中共有 13000000 行有效的 JSON 数据，0 行无效的 JSON 数据。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valid_lines, invalid_lines = check_jsonl_format(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d9ac12c-2098-4fa5-b8fb-415120b4d1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_line(file_path, output_path, invalid_line_num):\n",
    "    \"\"\"\n",
    "    读取文件，跳过指定的无效行，并将结果写入新文件\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as infile, open(output_path, 'wb') as outfile:\n",
    "        for idx, line in enumerate(infile):\n",
    "            if idx + 1 != invalid_line_num:  # 跳过无效行\n",
    "                outfile.write(line)\n",
    "\n",
    "# 使用该函数删除第 9598787 行并保存为新文件\n",
    "# remove_invalid_line('./dataset/mobvoi_seq_monkey_general_open_corpus.jsonl',\n",
    "#                     './dataset/mobvoi_seq_monkey_general_open_corpus_cleaned.jsonl', \n",
    "#                     invalid_line_num=9598787)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ed35dc-2071-4b83-bf55-54b5126b9c7c",
   "metadata": {},
   "source": [
    "- Step 5.定义处理函数（逐块处理数据）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a088ccd7-6d94-4f9e-9980-9fb2dfe5cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_process(chunk_size=50000):\n",
    "    chunk_idx = 0\n",
    "\n",
    "    with jsonlines.open('./dataset/mobvoi_seq_monkey_general_open_corpus/mobvoi_seq_monkey_general_open_corpus.jsonl') as reader:\n",
    "        with open('./dataset/pretrain_data_demo.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['text'])\n",
    "\n",
    "            while True:\n",
    "                chunk = list(itertools.islice(reader, chunk_size))\n",
    "                if not chunk:\n",
    "                    break\n",
    "\n",
    "                for idx, obj in enumerate(chunk):\n",
    "                    try:\n",
    "                        content = obj.get('text', '')\n",
    "                        if len(content) > 512:\n",
    "                            continue\n",
    "                        writer.writerow([content])\n",
    "                    except UnicodeDecodeError as e:\n",
    "                        print(f\"Skipping invalid line {chunk_idx * chunk_size + idx + 1}: {e}\")\n",
    "                        continue\n",
    "                chunk_idx += 1\n",
    "                print('chunk:', ((chunk_idx - 1) * chunk_size, chunk_idx * chunk_size), 'process end')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2bd87a",
   "metadata": {},
   "source": [
    "## Pretrain预训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece40cee-d5d0-4e07-b60d-394193e10724",
   "metadata": {},
   "source": [
    "- 模型预训练\n",
    "- 15个epoch预训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92bb6d6-7859-452c-a326-16acc2600b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !deepspeed --master_port 29500 --num_gpus=2 pretrain.py --epochs 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5d1e47-8334-4eaa-a147-f352b6c47819",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "import platform\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import optim\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from contextlib import nullcontext\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from model.model import Transformer\n",
    "from model.LMConfig import LMConfig\n",
    "from model.dataset import PretrainDataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def Logger(content):\n",
    "    if not ddp or dist.get_rank() == 0:\n",
    "        print(content)\n",
    "\n",
    "\n",
    "def get_lr(it, all):\n",
    "    warmup_iters = args.warmup_iters\n",
    "    lr_decay_iters = all\n",
    "    min_lr = args.learning_rate / 10\n",
    "\n",
    "    if it < warmup_iters:\n",
    "        return args.learning_rate * it / warmup_iters\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (args.learning_rate - min_lr)\n",
    "\n",
    "\n",
    "def train_epoch(epoch, wandb):\n",
    "    start_time = time.time()\n",
    "    for step, (X, Y, loss_mask) in enumerate(train_loader):\n",
    "        X = X.to(args.device)\n",
    "        Y = Y.to(args.device)\n",
    "        loss_mask = loss_mask.to(args.device)\n",
    "\n",
    "        lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        with ctx:\n",
    "            out = model(X, Y)\n",
    "            loss = out.last_loss / args.accumulation_steps\n",
    "            loss_mask = loss_mask.view(-1)\n",
    "            loss = torch.sum(loss * loss_mask) / loss_mask.sum()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % args.accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if step % args.log_interval == 0:\n",
    "            spend_time = time.time() - start_time\n",
    "            Logger(\n",
    "                'Epoch:[{}/{}]({}/{}) loss:{:.3f} lr:{:.7f} epoch_Time:{}min:'.format(\n",
    "                    epoch,\n",
    "                    args.epochs,\n",
    "                    step,\n",
    "                    iter_per_epoch,\n",
    "                    loss.item() * args.accumulation_steps,\n",
    "                    optimizer.param_groups[-1]['lr'],\n",
    "                    spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60))\n",
    "\n",
    "            if (wandb is not None) and (not ddp or dist.get_rank() == 0):\n",
    "                wandb.log({\"loss\": loss.item() * args.accumulation_steps,\n",
    "                           \"lr\": optimizer.param_groups[-1]['lr'],\n",
    "                           \"epoch_Time\": spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60})\n",
    "\n",
    "        if (step + 1) % args.save_interval == 0 and (not ddp or dist.get_rank() == 0):\n",
    "            model.eval()\n",
    "            moe_path = '_moe' if lm_config.use_moe else ''\n",
    "            ckp = f'{args.save_dir}/pretrain_{lm_config.dim}{moe_path}.pth'\n",
    "\n",
    "            if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "                state_dict = model.module.state_dict()\n",
    "            else:\n",
    "                state_dict = model.state_dict()\n",
    "\n",
    "            torch.save(state_dict, ckp)\n",
    "            model.train()\n",
    "\n",
    "\n",
    "def init_model():\n",
    "    def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('./model/minimind_tokenizer')\n",
    "\n",
    "    model = Transformer(lm_config).to(args.device)\n",
    "    # moe_path = '_moe' if lm_config.use_moe else ''\n",
    "\n",
    "    Logger(f'LLM总参数量：{count_parameters(model) / 1e6:.3f} 百万')\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def init_distributed_mode():\n",
    "    if not ddp: return\n",
    "    global ddp_local_rank, DEVICE\n",
    "\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    ddp_rank = int(os.environ[\"RANK\"])\n",
    "    ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    DEVICE = f\"cuda:{ddp_local_rank}\"\n",
    "    torch.cuda.set_device(DEVICE)\n",
    "\n",
    "\n",
    "# torchrun --nproc_per_node 2 1-pretrain.py\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"MiniMind Pretraining\")\n",
    "    parser.add_argument(\"--out_dir\", type=str, default=\"out\", help=\"Output directory\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=20, help=\"Number of epochs\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=64, help=\"Batch size\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=2e-4, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "                        help=\"Device to use\")\n",
    "    parser.add_argument(\"--dtype\", type=str, default=\"bfloat16\", help=\"Data type\")\n",
    "    parser.add_argument(\"--use_wandb\", action=\"store_true\", help=\"Use Weights & Biases\")\n",
    "    parser.add_argument(\"--wandb_project\", type=str, default=\"MiniMind-Pretrain\", help=\"Weights & Biases project name\")\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=1, help=\"Number of workers for data loading\")\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"./dataset/pretrain_data.csv\", help=\"Path to training data\")\n",
    "    parser.add_argument(\"--ddp\", action=\"store_true\", help=\"Use DistributedDataParallel\")\n",
    "    parser.add_argument(\"--accumulation_steps\", type=int, default=8, help=\"Gradient accumulation steps\")\n",
    "    parser.add_argument(\"--grad_clip\", type=float, default=1.0, help=\"Gradient clipping threshold\")\n",
    "    parser.add_argument(\"--warmup_iters\", type=int, default=0, help=\"Number of warmup iterations\")\n",
    "    parser.add_argument(\"--log_interval\", type=int, default=100, help=\"Logging interval\")\n",
    "    parser.add_argument(\"--save_interval\", type=int, default=1000, help=\"Model saving interval\")\n",
    "    parser.add_argument('--local_rank', type=int, default=-1, help='local rank for distributed training')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    lm_config = LMConfig()\n",
    "    max_seq_len = lm_config.max_seq_len\n",
    "    args.save_dir = os.path.join(args.out_dir)\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "    tokens_per_iter = args.batch_size * max_seq_len\n",
    "    torch.manual_seed(1337)\n",
    "    device_type = \"cuda\" if \"cuda\" in args.device else \"cpu\"\n",
    "\n",
    "    args.wandb_run_name = f\"MiniMind-Pretrain-Epoch-{args.epochs}-BatchSize-{args.batch_size}-LearningRate-{args.learning_rate}\"\n",
    "\n",
    "    ctx = nullcontext() if device_type == \"cpu\" else torch.cuda.amp.autocast()\n",
    "\n",
    "    ddp = int(os.environ.get(\"RANK\", -1)) != -1  # is this a ddp run?\n",
    "    ddp_local_rank, DEVICE = 0, \"cuda:0\"\n",
    "    if ddp:\n",
    "        init_distributed_mode()\n",
    "        args.device = torch.device(DEVICE)\n",
    "\n",
    "    if args.use_wandb and (not ddp or ddp_local_rank == 0):\n",
    "        import wandb\n",
    "\n",
    "        wandb.init(project=args.wandb_project, name=args.wandb_run_name)\n",
    "    else:\n",
    "        wandb = None\n",
    "\n",
    "    model, tokenizer = init_model()\n",
    "    df = pd.read_csv(args.data_path)\n",
    "    df = df.sample(frac=1.0)\n",
    "    train_ds = PretrainDataset(df, tokenizer, max_length=max_seq_len)\n",
    "    train_sampler = DistributedSampler(train_ds) if ddp else None\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_workers,\n",
    "        sampler=train_sampler\n",
    "    )\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype in ['float16', 'bfloat16']))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    if False and platform.system() != 'Windows' and float(torch.__version__.split('.')[0]) >= 2:\n",
    "        Logger(\"compiling the model... (takes a ~minute)\")\n",
    "        unoptimized_model = model\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    if ddp:\n",
    "        model._ddp_params_and_buffers_to_ignore = {\"pos_cis\"}\n",
    "        model = DistributedDataParallel(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "    iter_per_epoch = len(train_loader)\n",
    "    for epoch in range(args.epochs):\n",
    "        train_epoch(epoch, wandb)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c203a2-b0d9-40f6-b07b-803db3ba875b",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0649752b-e5ca-42ff-81ec-bf0afd0b0762",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/0d46acea891701fd573115782288e05.jpg\" alt=\"0d46acea891701fd573115782288e05\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d524605-a791-4704-9626-62ea18926667",
   "metadata": {},
   "source": [
    "训练结束"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3114ad-174c-4fce-8fbc-4abd657f19ab",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/9f1b2321dc79550b1e0f194e165ff22.png\" alt=\"9f1b2321dc79550b1e0f194e165ff22\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4892c1a",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7184bee3-759b-4bdd-87f4-de51abb14e69",
   "metadata": {},
   "source": [
    "测试运行（单字符预测）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8eaf2280-983a-4fb8-9b34-27c895b1c3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的模块\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from model.model import Transformer  # 确保路径正确\n",
    "from model.LMConfig import LMConfig   # 导入 LMConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71d917b1-dd6a-4211-bcab-3026ae665e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建配置对象\n",
    "lm_config = LMConfig(\n",
    "    dim=512,                # 模型的维度\n",
    "    n_layers=8,            # 层数\n",
    "    n_heads=16,            # 注意力头数\n",
    "    vocab_size=6400,       # 词汇表大小\n",
    "    max_seq_len=512,       # 最大序列长度\n",
    "    dropout=0.1            # Dropout 概率\n",
    "    # 这里可以添加更多配置，根据需要进行调整\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e5993ce-8479-4c73-abe7-69c7cad07c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 Transformer 模型\n",
    "model = Transformer(lm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff44e2ab-2b33-4d3a-8f84-082d84f74580",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a22c151-5f07-44c7-a44a-bdb9de35a1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "487f2dc2-4469-4abb-8503-e9613bb594ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (tok_embeddings): Embedding(6400, 512)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0-7): 8 x TransformerBlock(\n",
      "      (attention): Attention(\n",
      "        (wq): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (wk): Linear(in_features=512, out_features=256, bias=False)\n",
      "        (wv): Linear(in_features=512, out_features=256, bias=False)\n",
      "        (wo): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attention_norm): RMSNorm()\n",
      "      (ffn_norm): RMSNorm()\n",
      "      (feed_forward): FeedForward(\n",
      "        (w1): Linear(in_features=512, out_features=1408, bias=False)\n",
      "        (w2): Linear(in_features=1408, out_features=512, bias=False)\n",
      "        (w3): Linear(in_features=512, out_features=1408, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      "  (output): Linear(in_features=512, out_features=6400, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "# 检查模型结构和参数\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f14b56c-3699-42c7-9a93-54a6db485ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): Embedding(6400, 512)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-7): 8 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (wk): Linear(in_features=512, out_features=256, bias=False)\n",
       "        (wv): Linear(in_features=512, out_features=256, bias=False)\n",
       "        (wo): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): Linear(in_features=512, out_features=1408, bias=False)\n",
       "        (w2): Linear(in_features=1408, out_features=512, bias=False)\n",
       "        (w3): Linear(in_features=512, out_features=1408, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=512, out_features=6400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型权重\n",
    "model.load_state_dict(torch.load('../out/pretrain_512.pth', map_location=device))\n",
    "model.eval()  # 切换到评估模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1d28830-f20c-4ddd-b992-3ccd2ae14be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备输入文本\n",
    "input_text = \"你好，好久不\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e6e7069-e23b-4400-ad59-d69a7ea30359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "见\n"
     ]
    }
   ],
   "source": [
    "# 生成输出\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "    generated_ids = output.logits.argmax(dim=-1)  # 假设使用 argmax 选择输出\n",
    "    generated_text = tokenizer.decode(generated_ids[0])\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fd37a0-ef5f-4028-9530-ea00bad69c3b",
   "metadata": {},
   "source": [
    "测试运行（多字符预测）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef773698-ca74-4d69-84df-4d9d88b6d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备输入文本\n",
    "input_text = \"长江、\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7c78358-e3bb-4884-bbdf-1dc263018637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "黄河\n"
     ]
    }
   ],
   "source": [
    "# 生成多个 token\n",
    "num_tokens_to_generate = 2  # 要生成的 token 数量\n",
    "generated_tokens = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_tokens_to_generate):\n",
    "        output = model(input_ids)\n",
    "        next_token = output.logits.argmax(dim=-1)[:, -1]  # 获取最后一个 token 的预测\n",
    "        generated_tokens.append(next_token.item())  # 将 token ID 添加到列表中\n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)  # 将新 token 添加到输入中\n",
    "\n",
    "# 将生成的 token IDs 转换为文本\n",
    "generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "# 打印最终回复\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5279f206-d115-4695-9746-764ee0d1b725",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共产党的领导下，中国共产党人坚定不移走中国特色社会主义道路，为实现中华民族伟大复兴的中国梦而不懈奋斗。2、在使用过程中，应注意以下几点：\n",
      "1、在使用过程中，应注意避免阳光直射，以免灼伤皮肤。\n",
      "2、在使用过程中，应注意避免阳光直射，以免�\n"
     ]
    }
   ],
   "source": [
    "# 准备输入文本\n",
    "input_text = \"中国\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "# 生成多个 token\n",
    "num_tokens_to_generate = 100  # 要生成的 token 数量\n",
    "generated_tokens = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_tokens_to_generate):\n",
    "        output = model(input_ids)\n",
    "        next_token = output.logits.argmax(dim=-1)[:, -1]  # 获取最后一个 token 的预测\n",
    "        generated_tokens.append(next_token.item())  # 将 token ID 添加到列表中\n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)  # 将新 token 添加到输入中\n",
    "\n",
    "# 将生成的 token IDs 转换为文本\n",
    "generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "# 打印最终回复\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8990bbc-7625-4c5e-8020-3c4e8890518b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db174a95",
   "metadata": {},
   "source": [
    "## Eval Pretrain's Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7e4e2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型参数: 26.878464 百万 = 0.026878464 B (Billion)\n",
      "问题： 椭圆和圆的区别\n",
      "回答：是很大的区别。\n",
      "椭圆的区别\n",
      "椭圆的区别是椭圆的区别，椭圆的区别在于椭圆的区别。椭圆形状是椭圆形，椭圆形是椭圆形，椭圆形是椭圆形，椭圆形是椭圆形。椭圆形的区别在于椭圆形的区别。椭圆形椭圆形的区别在于椭圆形的区别。椭圆形的区别是椭圆形状的区别。椭圆形的区别是椭圆形的区别。椭圆形的区别是椭圆形的区别。椭圆形的区别是椭圆形的区别。椭圆形的区别就是椭圆形的区别。椭圆形的区别是椭圆形的区别。椭圆形的区别是椭圆形的区别。椭圆形的区别是椭圆形的区别。椭圆形的区别是椭圆形的区别。椭圆形的区别是椭圆形状的区别。椭圆形的区别是椭圆形状的区别。椭圆形的区别是椭圆形的区别。椭圆形的区别是椭圆形的区别。椭圆形的区别是椭圆形的区别。椭圆形的区别是椭圆形的区别。椭圆形的区别是椭圆形的区别。椭圆形的区别是椭圆形的区别。椭圆形的区别是椭圆形的区别。椭圆形的区别是椭圆形的区别。椭圆形的区别是椭圆形的区别。椭圆形的区别主要是椭圆形的区别，椭圆形的区别是椭圆形的区别。椭圆形的区别是椭圆形的区别。椭圆形的区别是椭圆形的区别\n",
      "\n",
      "8.02924919128418 s\n",
      "问题： 中国关于马克思主义基本原理\n",
      "回答：论坛”在北京举行。\n",
      "\n",
      "0.140608549118042 s\n",
      "问题： 人类大脑的主要功能是\n",
      "回答：通过脑细胞和脑神经的功能来实现，而大脑神经是大脑神经的一个系统，而神经是大脑神经的发育和生理过程。\n",
      "1、神经递质\n",
      "神经递质是人类大脑神经递质中重要成分，是人脑神经递质的主要成分。当神经递质过多时，会影响到神经递质，从而诱发神经递质的增加，从而导致大脑神经递质的增多，从而加重神经递质的增加。\n",
      "2、神经递质\n",
      "神经递质是神经递质，是神经递质中一种，也是神经递质中一部分的一种。神经递质在神经递质中起着重要作用，当神经递质过多时，会影响神经递质的增多，从而影响到神经递质的进展，导致神经递质增多，从而诱发神经递质的增加。\n",
      "3、神经递质\n",
      "神经递质是神经递质的一种，它是一种神经递质，是一种多态的物质，在神经递质的作用下，会释放出来一种叫做神经递质的物质。\n",
      "4、神经递质\n",
      "神经递质可以促进神经递质的增多，从而诱发神经递质增多。\n",
      "\n",
      "5.374856948852539 s\n",
      "问题： 万有引力是\n",
      "回答：地球上最大的引力，它是地球上最大的点的，也是世界上最大的引力，它也是一个大的模糊的世界。万有引力是地球上最大的引力，它是地球上最大的引力。\n",
      "\n",
      "0.7337100505828857 s\n",
      "问题： 世界上人口最多的国家是\n",
      "回答：印度，印度、巴西、马来西亚和美国。\n",
      "世界上人口最多的国家是印度，印度是世界上人口最多的国家之一。\n",
      "世界上人口最多的国家是印度，印度，马来西亚，巴西，印度，马来西亚，马来西亚，泰国，新加坡，马来西亚，俄罗斯，印度，马来西亚，印度，泰国，新加坡，新加坡，新西兰，新加坡，马来西亚，泰国，马来西亚，马来西亚，新加坡，新加坡，印度，泰国，马来西亚，新西兰，马来西亚，新加坡，新加坡，马来西亚，泰国，新加坡，新西兰，新加坡，马来西亚，新加坡，西班牙，新加坡，马来西亚，新西兰，新加坡，新加坡，马来西亚，泰国，马来西亚，泰国，新加坡，新加坡，新加坡，新加坡，马来西亚，马来西亚，新加坡，新加坡，马来西亚，新加坡，泰国，新加坡，新加坡，新西兰，马来西亚，阿拉伯，新加坡，新加坡，新加坡，西班牙，新加坡，泰国，印度，马来西亚，新加坡，新加坡，新加坡，新加坡，马来西亚，泰国，新西兰，新加坡，新加坡，新加坡，新加坡，马来西亚，马来西亚，新加坡，新加坡，新加坡，新加坡，马来西亚，马来西亚，泰国，新加坡，新加坡，新加坡，\n",
      "\n",
      "8.427174091339111 s\n",
      "问题： DNA的全称是\n",
      "回答：“非金属矿物”，是我国矿物学的重要组成部分。\n",
      "DNA分子的结构是基于分子分子的结构特征，可以分为分子结构和结构。根据分子结构的不同，DNA分子可分为：\n",
      "1. α-β-羟基-2-羟基-3-氨基-羰基-1-羰基-2-羰基-羰基-2-羰基-4-羰基-羰基-2-羰基-2-羰基-2-羰基-羰基基-羰基-1-羰基-2-羰基-2-羰基-2-羰基-羰基-2羰基-3-羰基-2-羰基-羰基-3-羰基-羰基-羰基基-羰基-2羰基-羰基羰基-NN-羰基羰基-羰基羰基基丙氧基-NN-羰基氨基丙氧基乙基羰基羰基基酰基乙炔基团。\n",
      "\n",
      "4.565714120864868 s\n",
      "问题： 数学中π的值大约是\n",
      "回答：“1”和“1”的值的差值，而数学中π的值大约是“3”和“1”的值。\n",
      "数学中π为整数，也就是“0”和“1”。\n",
      "数学中π是个个数，它是个个数，它就是个个数，它是个数，这个值就是这个值，它是个个数，它是个一个个数，它的大小，它的大小，它的大小，它的个数，这个值就是这个值，它是个个数，它的大小，它的个数就是这个值，它是个个数，它是个个数，它就是个个数了。\n",
      "数学中π的值大约是“1”，就是这个值，它就是个个数，它叫做一个数，它就是这个值，它是个数，它就是这个值，它是个个数，它就是这个值，它是个个数，它就是这个数，它是个数，它是个个数，它是个个数，它的一个数是个个，它就是这个数，它是个数，它是个个数，它是个个数，它就是这个数。\n",
      "\n",
      "4.382440090179443 s\n",
      "问题： 世界上最高的山峰是\n",
      "回答：中国最大的白鹿峡，也是世界上最大的白鹿峡。这座峡谷内的峡谷，是世界上最大的白鹿峡，也是世界上最高的山峰。\n",
      "\n",
      "0.8064529895782471 s\n",
      "问题： 太阳系中最大的行星是\n",
      "回答：太阳系中的一个行星。太阳系中行星的形状很像太阳系，它是太阳系中最大的行星。太阳系中行星的形状非常像太阳系，其大小为2896788个，相当于地球的大小（约为6.675936个），这颗行星的形状非常像地球的行星，它是太阳系中最大的行星。\n",
      "\n",
      "1.2703766822814941 s\n",
      "问题： 二氧化碳的化学分子式是\n",
      "回答：Na2O·SO2。\n",
      "三氧化二氧有机化合物，是人体代谢的有机化合物之一。\n",
      "四氢呋喃是无机化合物，是重要的有机化合物之一，是重要的工业化合物，是工业生产中不可缺少的物质，可用于制造工业、食品工业、医药工业、造纸工业等。\n",
      "\n",
      "1.5829417705535889 s\n",
      "问题： 地球上最大的动物是\n",
      "回答：人类，他们在人类的世界中是最美的。\n",
      "地球上的动物是人类的家园，也是世界的动物。人类在人类生存的途中，会受到动物的各种影响，比如动物、植物、动物、植物等。\n",
      "人类最大的动物是地球上最大的动物，最大的动物是人类，最大的动物是人类。\n",
      "地球上有许多动物，包括动物和植物，包括动物、植物和动物。\n",
      "人类最大的动物是地球上的动物，最大的动物是地球上的动物。\n",
      "地球上的动物，最大的动物是地球上的动物，最大的动物是地球上的动物。\n",
      "地球上有很多动物，包括动物、植物和动物。\n",
      "地球上的动物包括动物、植物和动物。这些动物都来自地球。\n",
      "动物是世界的主人。\n",
      "地球上有很多动物，包括动物、植物、植物和动物。\n",
      "地球上的动物包括动物。\n",
      "人类最大的动物是地球上的动物。\n",
      "地球上的动物包括动物和植物。\n",
      "地球上有很多动物，包括动物。\n",
      "动物和植物。\n",
      "它们是地球上所有动物。\n",
      "动物是人类的家园，是人类的家园。\n",
      "它们被称为动物。\n",
      "它们是人类和动物的家。\n",
      "它们的身份和它们之间存在着关系。\n",
      "它们是人类的主要食物。它们通常是动物和植物。\n",
      "它们是人类和动物的家园。\n",
      "它们通常是动物。\n",
      "它们是动物和植物。它们是动物。它们是动物。它们有很强的吸收能力。\n",
      "它们可以吸收和吸收它们。它们可以吸收和吸收它们。它们可以吸收它们。\n",
      "它们可以吸收和吸收它们。它们可以吸收它们。它们通常是动物和植物。\n",
      "它们非常有趣。\n",
      "\n",
      "6.8042778968811035 s\n",
      "问题： 地球自转一圈大约需要\n",
      "回答：10天的时间，这时就需要人工的力量，人工的力量，就需要人工的力量。\n",
      "在地球的大部分时间都要经过一个月的训练，才能完成。\n",
      "在地球的大部分时间，地球的大部分时间都是可以做到的，所以地球的大部分时间都是可以做到的。\n",
      "在地球上，地球的大部分时间都需要做到，所以地球上，地球的大部分时间都会做到，所以地球上有了大部分的时间才会做到，所以地球上没有什么大的空间。\n",
      "地球上还有很多地方，比如：地表水、地下水，以及水体，这些地方都是可以做到的。\n",
      "地球上的大部分时间都是可以做到的，所以地球上有了大部分的时间都要做到，所以地球上有了大部分的时间才会做到，所以地球上有了大部分的时间才会做到。\n",
      "地球上有了大部分的时间才会做到，所以地球上有了大部分的时间才会做到。\n",
      "地球上有了大部分的时间都要做到，所以地球上有了大部分的时间才会做到。\n",
      "\n",
      "4.481493711471558 s\n",
      "问题： 杭州市的美食有\n",
      "回答：哪些?\n",
      "杭州美食有哪些\n",
      "杭州美食有哪些\n",
      "杭州美食有哪些?杭州美食有哪些呢?\n",
      "杭州美食有哪些\n",
      "杭州美食有哪些?\n",
      "杭州美食有哪些\n",
      "杭州美食有哪些?\n",
      "杭州美食有哪些?杭州美食有哪些?\n",
      "杭州美食有哪些?美杭州美食有哪些?\n",
      "杭州美食有哪些\n",
      "杭州美食有哪些?杭州美食有哪些?\n",
      "杭州美食有哪些?美杭州美食有哪些?美杭州美食有哪些?\n",
      "杭州美杏美品有哪些?\n",
      "杭州美杏美品有哪些?美杜仲\n",
      "美杜仲是一种中药材，具有补肺益气、养血生肌、补血益气、益肾壮阳等功效，适用于肺虚咳嗽、肺痈结痈、肺热咳嗽、肺结肻等病证。\n",
      "杭州美杜仲的功效\n",
      "杭州美杜仲的功效:\n",
      "杭州美杜仲是一种中药材，具有补肺益胃的功效，对肺肾虚咳嗽、肺痈肝热、肝胆湿热、肺结核、肺燥咳嗽、胆结石、肺结核、肺肿痛、肺结核、胃病等病证都有治疗效果。\n",
      "杭州美杜仲的功效:\n",
      "美杜仲是中药材中的珍品，具有补肺益胃的功效，可以补肺益肾，促进肝脾健脾，可以缓解腹部不适，还可以促进肾脏的解毒，对于肺结核有很好的作用，还可以治疗肺结核和肾炎，对肺结核有一定的疗效。\n",
      "杭州美杜仲的功效:\n",
      "\n",
      "\n",
      "8.613029718399048 s\n",
      "问题： 江苏省的最好的大学\n",
      "回答：之一，在江苏省中部地区，在江苏、浙江、江西，东部和南部与江苏、浙江、安徽、湖北、河南、河北、山东、山西、陕西、甘肃、宁夏等省市相邻。\n",
      "江苏省的最好的大学之一，在江苏省中西部地区拥有最好的天然溶洞，也是最好的天然溶洞。江苏大学是国家“211工程”重点建设大学，国家重点建设大学，国家首批“211工程”和“985工程”重点建设高校，“211工程”和“985工程”重点建设高校。\n",
      "江苏高教部长江学者奖学金为国家“211工程”和“985工程”重点建设高校，国家“985工程”和“211工程”重点建设高校。\n",
      "江苏大学是国家“211工程”和“985工程”重点建设大学，国家“211工程”和“985工程”重点建设高校，国家首批“211工程”和“985工程”重点建设高校。江苏大学是国家“211工程”和“985工程”重点建设高校，国家“211工程”和“985工程”重点建设高校。江苏大学是国家“211工程”和“985工程”重点建设高校，国家首批国家示范性大学。\n",
      "\n",
      "7.000101566314697 s\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from model.model import Transformer\n",
    "from model.LMConfig import LMConfig\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def init_model(lm_config):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('../model/mateconv_tokenizer')\n",
    "    model_from = 1  # 1从权重，2用transformers\n",
    "\n",
    "    if model_from == 1:\n",
    "        moe_path = '_moe' if lm_config.use_moe else ''\n",
    "        ckp = f'../out/pretrain_{lm_config.dim}{moe_path}.pth'\n",
    "\n",
    "        model = Transformer(lm_config)\n",
    "        state_dict = torch.load(ckp, map_location=device)\n",
    "\n",
    "        # 处理不需要的前缀\n",
    "        unwanted_prefix = '_orig_mod.'\n",
    "        for k, v in list(state_dict.items()):\n",
    "            if k.startswith(unwanted_prefix):\n",
    "                state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "\n",
    "        for k, v in list(state_dict.items()):\n",
    "            if 'mask' in k:\n",
    "                del state_dict[k]\n",
    "\n",
    "        # 加载到模型中\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained('minimind', trust_remote_code=True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    print(f'模型参数: {count_parameters(model) / 1e6} 百万 = {count_parameters(model) / 1e9} B (Billion)')\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "    random.seed(seed)  # 设置 Python 的随机种子\n",
    "    np.random.seed(seed)  # 设置 NumPy 的随机种子\n",
    "    torch.manual_seed(seed)  # 设置 PyTorch 的随机种子\n",
    "    torch.cuda.manual_seed(seed)  # 为当前 GPU 设置随机种子（如果有）\n",
    "    torch.cuda.manual_seed_all(seed)  # 为所有 GPU 设置随机种子（如果有）\n",
    "    torch.backends.cudnn.deterministic = True  # 确保每次返回的卷积算法是确定的\n",
    "    torch.backends.cudnn.benchmark = False  # 关闭 cuDNN 的自动调优，避免不确定性\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # -----------------------------------------------------------------------------\n",
    "    out_dir = 'out'\n",
    "    start = \"\"\n",
    "    temperature = 0.7\n",
    "    top_k = 8\n",
    "    setup_seed(1337)\n",
    "    # device = 'cpu'\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    dtype = 'bfloat16'\n",
    "    max_seq_len = 512\n",
    "    lm_config = LMConfig()\n",
    "    lm_config.max_seq_len = max_seq_len\n",
    "    # -----------------------------------------------------------------------------\n",
    "\n",
    "    model, tokenizer = init_model(lm_config)\n",
    "    model = model.eval()\n",
    "    # int(input('输入0自动测试，输入1问题测试：'))\n",
    "    answer_way = 0\n",
    "    stream = True\n",
    "\n",
    "    prompt_datas = [\n",
    "        '椭圆和圆的区别',\n",
    "        '中国关于马克思主义基本原理',\n",
    "        '人类大脑的主要功能是',\n",
    "        '万有引力是',\n",
    "        '世界上人口最多的国家是',\n",
    "        'DNA的全称是',\n",
    "        '数学中π的值大约是',\n",
    "        '世界上最高的山峰是',\n",
    "        '太阳系中最大的行星是',\n",
    "        '二氧化碳的化学分子式是',\n",
    "        '地球上最大的动物是',\n",
    "        '地球自转一圈大约需要',\n",
    "        '杭州市的美食有',\n",
    "        '江苏省的最好的大学',\n",
    "    ]\n",
    "\n",
    "    qa_index = 0\n",
    "    while True:\n",
    "        start = time.time()\n",
    "        if answer_way == 1:\n",
    "            # run generation\n",
    "            prompt = input('用户：')\n",
    "        else:\n",
    "            if qa_index >= len(prompt_datas):\n",
    "                break\n",
    "            prompt = prompt_datas[qa_index]\n",
    "            print('问题：', prompt)\n",
    "            qa_index += 1\n",
    "\n",
    "        prompt = tokenizer.bos_token + prompt\n",
    "        x = tokenizer(prompt).data['input_ids']\n",
    "        x = (torch.tensor(x, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            res_y = model.generate(x, tokenizer.eos_token_id, max_new_tokens=max_seq_len, temperature=temperature,\n",
    "                                   top_k=top_k, stream=stream)\n",
    "            print('回答：', end='')\n",
    "            try:\n",
    "                y = next(res_y)\n",
    "            except StopIteration:\n",
    "                print(\"No answer\")\n",
    "                continue\n",
    "\n",
    "            history_idx = 0\n",
    "            while y != None:\n",
    "                answer = tokenizer.decode(y[0].tolist())\n",
    "                if answer and answer[-1] == '�':\n",
    "                    try:\n",
    "                        y = next(res_y)\n",
    "                    except:\n",
    "                        break\n",
    "                    continue\n",
    "                # print(answer)\n",
    "                if not len(answer):\n",
    "                    try:\n",
    "                        y = next(res_y)\n",
    "                    except:\n",
    "                        break\n",
    "                    continue\n",
    "\n",
    "                print(answer[history_idx:], end='', flush=True)\n",
    "                try:\n",
    "                    y = next(res_y)\n",
    "                except:\n",
    "                    break\n",
    "                history_idx = len(answer)\n",
    "                if not stream:\n",
    "                    break\n",
    "\n",
    "            print('\\n')\n",
    "\n",
    "        end = time.time()\n",
    "        print(end - start, 's')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0cec9b-2faf-4fe3-a74d-f1046c16088a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "- 备用代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc1df13",
   "metadata": {},
   "source": [
    "data_process.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4b3963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import re\n",
    "import json\n",
    "import jsonlines\n",
    "import psutil\n",
    "import ujson\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "bos_token = \"<s>\"\n",
    "eos_token = \"</s>\"\n",
    "\n",
    "\n",
    "def pretrain_process(chunk_size=50000):\n",
    "    chunk_idx = 0\n",
    "\n",
    "    with jsonlines.open('./dataset/mobvoi_seq_monkey_general_open_corpus/mobvoi_seq_monkey_general_open_corpus.jsonl') as reader:\n",
    "        with open('./dataset/pretrain_data_demo.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['text'])\n",
    "\n",
    "            while True:\n",
    "                chunk = list(itertools.islice(reader, chunk_size))\n",
    "                if not chunk:\n",
    "                    break\n",
    "\n",
    "                for idx, obj in enumerate(chunk):\n",
    "                    try:\n",
    "                        content = obj.get('text', '')\n",
    "                        if len(content) > 512:\n",
    "                            continue\n",
    "                        writer.writerow([content])\n",
    "                    except UnicodeDecodeError as e:\n",
    "                        print(f\"Skipping invalid line {chunk_idx * chunk_size + idx + 1}: {e}\")\n",
    "                        continue\n",
    "                chunk_idx += 1\n",
    "                print('chunk:', ((chunk_idx - 1) * chunk_size, chunk_idx * chunk_size), 'process end')\n",
    "\n",
    "\n",
    "def sft_process(contain_history=False):\n",
    "    file_name = 'sft_data.csv'\n",
    "    if not contain_history:\n",
    "        file_name = 'sft_data_single.csv'\n",
    "\n",
    "    def chinese_ratio(text):\n",
    "        # 匹配所有中文字符\n",
    "        chinese_chars = re.findall(r'[\\u4e00-\\u9fff]', text)\n",
    "        # 中文字符数量占比\n",
    "        return len(chinese_chars) / len(text) if text else 0\n",
    "\n",
    "    def process_and_write_data(data):\n",
    "        q_lst, a_lst, history_lst = [], [], []\n",
    "        for per in data:\n",
    "            history, q, a = per['history'], per['q'], per['a']\n",
    "\n",
    "            if (contain_history and not history) or not q or not a:\n",
    "                continue\n",
    "            if len(q) < 10 or len(a) < 5:\n",
    "                continue\n",
    "            if len(q) > 512 or len(a) > 512:\n",
    "                continue\n",
    "            # 判断q和a中中文字符占比是否超过70%\n",
    "            if not (chinese_ratio(q) > 0.5 and chinese_ratio(a) > 0.5):\n",
    "                continue\n",
    "\n",
    "            q_lst.append(q)\n",
    "            a_lst.append(a)\n",
    "            if contain_history:\n",
    "                history_lst.append(history)\n",
    "            else:\n",
    "                history_lst.append([])\n",
    "\n",
    "        # 创建DataFrame并追加到CSV文件\n",
    "        df = pd.DataFrame({'history': history_lst, 'q': q_lst, 'a': a_lst})\n",
    "        # # 1、默认\n",
    "        # df.to_csv(f'./dataset/{file_name}', mode='a', header=False, index=False, lineterminator='\\r\\n', encoding='utf-8')\n",
    "        # 2、若遇到数据 `_csv.Error: need to escape, but no escapechar set` 问题，可加 escapechar='\\\\' 参数：\n",
    "        df.to_csv(f'./dataset/{file_name}', mode='a', header=False, index=False, lineterminator='\\r\\n', escapechar='\\\\',\n",
    "                  encoding='utf-8')\n",
    "\n",
    "    chunk_size = 1000  # 每次处理的记录数\n",
    "    data = []\n",
    "\n",
    "    with open(f'./dataset/{file_name}', 'w', encoding='utf-8') as f:\n",
    "        f.write('history,q,a\\n')\n",
    "\n",
    "    sft_datasets = ['./dataset/sft_data_zh.jsonl']\n",
    "    if not contain_history:\n",
    "        sft_datasets = ['./dataset/sft_data_zh.jsonl']\n",
    "\n",
    "    chunk_num = 0\n",
    "    for path in sft_datasets:\n",
    "        with jsonlines.open(path) as reader:\n",
    "            for idx, obj in enumerate(reader):\n",
    "                try:\n",
    "                    data.append({\n",
    "                        'history': obj.get('history', ''),\n",
    "                        'q': obj.get('input', '') + obj.get('q', ''),\n",
    "                        'a': obj.get('output', '') + obj.get('a', '')\n",
    "                    })\n",
    "\n",
    "                    if len(data) >= chunk_size:\n",
    "                        chunk_num += 1\n",
    "                        process_and_write_data(data)\n",
    "                        data = []\n",
    "                        if chunk_num % 100 == 0:\n",
    "                            print(f'chunk:{chunk_num} process end')\n",
    "                except jsonlines.InvalidLineError as e:\n",
    "                    print(f\"Skipping invalid JSON line {idx + 1}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            if data:\n",
    "                process_and_write_data(data)\n",
    "                data = []\n",
    "\n",
    "\n",
    "def rl_process():\n",
    "    ################\n",
    "    # Dataset\n",
    "    ################\n",
    "\n",
    "    dataset_paths = [\n",
    "        './dataset/dpo/dpo_zh_demo.json',\n",
    "        './dataset/dpo/dpo_train_data.json',\n",
    "        './dataset/dpo/huozi_rlhf_data.json',\n",
    "    ]\n",
    "\n",
    "    train_dataset = load_dataset('json', data_files=dataset_paths)\n",
    "\n",
    "    merged_data = []\n",
    "    for split in train_dataset.keys():\n",
    "        merged_data.extend(train_dataset[split])\n",
    "\n",
    "    with open('./dataset/dpo/train_data.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(merged_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained('./model/mateconv_tokenizer', use_fast=False)\n",
    "    print('tokenizer词表大小：', len(tokenizer))\n",
    "\n",
    "    ################\n",
    "    # 1: pretrain\n",
    "    # 2: sft\n",
    "    # 3: RL\n",
    "    ################\n",
    "    process_type = 1\n",
    "\n",
    "    if process_type == 1:\n",
    "        pretrain_process()\n",
    "    if process_type == 2:\n",
    "        sft_process(contain_history=False)\n",
    "    if process_type == 3:\n",
    "        rl_process() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4117965",
   "metadata": {},
   "source": [
    "pretrain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ca33cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import optim\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from contextlib import nullcontext\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from model.model import Transformer\n",
    "from model.LMConfig import LMConfig\n",
    "from model.dataset import PretrainDataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def Logger(content):\n",
    "    if not ddp or dist.get_rank() == 0:\n",
    "        print(content)\n",
    "\n",
    "\n",
    "def get_lr(it, all):\n",
    "    warmup_iters = args.warmup_iters\n",
    "    lr_decay_iters = all\n",
    "    min_lr = args.learning_rate / 10\n",
    "\n",
    "    if it < warmup_iters:\n",
    "        return args.learning_rate * it / warmup_iters\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (args.learning_rate - min_lr)\n",
    "\n",
    "\n",
    "def train_epoch(epoch, wandb):\n",
    "    start_time = time.time()\n",
    "    for step, (X, Y, loss_mask) in enumerate(train_loader):\n",
    "        X = X.to(args.device)\n",
    "        Y = Y.to(args.device)\n",
    "        loss_mask = loss_mask.to(args.device)\n",
    "\n",
    "        lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        with ctx:\n",
    "            out = model(X, Y)\n",
    "            loss = out.last_loss / args.accumulation_steps\n",
    "            loss_mask = loss_mask.view(-1)\n",
    "            loss = torch.sum(loss * loss_mask) / loss_mask.sum()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % args.accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if step % args.log_interval == 0:\n",
    "            spend_time = time.time() - start_time\n",
    "            Logger(\n",
    "                'Epoch:[{}/{}]({}/{}) loss:{:.3f} lr:{:.7f} epoch_Time:{}min:'.format(\n",
    "                    epoch,\n",
    "                    args.epochs,\n",
    "                    step,\n",
    "                    iter_per_epoch,\n",
    "                    loss.item() * args.accumulation_steps,\n",
    "                    optimizer.param_groups[-1]['lr'],\n",
    "                    spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60))\n",
    "\n",
    "            if (wandb is not None) and (not ddp or dist.get_rank() == 0):\n",
    "                wandb.log({\"loss\": loss.item() * args.accumulation_steps,\n",
    "                           \"lr\": optimizer.param_groups[-1]['lr'],\n",
    "                           \"epoch_Time\": spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60})\n",
    "\n",
    "        if (step + 1) % args.save_interval == 0 and (not ddp or dist.get_rank() == 0):\n",
    "            model.eval()\n",
    "            moe_path = '_moe' if lm_config.use_moe else ''\n",
    "            ckp = f'{args.save_dir}/pretrain_{lm_config.dim}{moe_path}.pth'\n",
    "\n",
    "            if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "                state_dict = model.module.state_dict()\n",
    "            else:\n",
    "                state_dict = model.state_dict()\n",
    "\n",
    "            torch.save(state_dict, ckp)\n",
    "            model.train()\n",
    "\n",
    "\n",
    "def init_model():\n",
    "    def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('./model/minimind_tokenizer')\n",
    "\n",
    "    model = Transformer(lm_config).to(args.device)\n",
    "    # moe_path = '_moe' if lm_config.use_moe else ''\n",
    "\n",
    "    Logger(f'LLM总参数量：{count_parameters(model) / 1e6:.3f} 百万')\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def init_distributed_mode():\n",
    "    if not ddp: return\n",
    "    global ddp_local_rank, DEVICE\n",
    "\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    ddp_rank = int(os.environ[\"RANK\"])\n",
    "    ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    DEVICE = f\"cuda:{ddp_local_rank}\"\n",
    "    torch.cuda.set_device(DEVICE)\n",
    "\n",
    "\n",
    "# torchrun --nproc_per_node 2 1-pretrain.py\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"MiniMind Pretraining\")\n",
    "    parser.add_argument(\"--out_dir\", type=str, default=\"out\", help=\"Output directory\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=20, help=\"Number of epochs\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=64, help=\"Batch size\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=2e-4, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "                        help=\"Device to use\")\n",
    "    parser.add_argument(\"--dtype\", type=str, default=\"bfloat16\", help=\"Data type\")\n",
    "    parser.add_argument(\"--use_wandb\", action=\"store_true\", help=\"Use Weights & Biases\")\n",
    "    parser.add_argument(\"--wandb_project\", type=str, default=\"MiniMind-Pretrain\", help=\"Weights & Biases project name\")\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=1, help=\"Number of workers for data loading\")\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"./dataset/pretrain_data.csv\", help=\"Path to training data\")\n",
    "    parser.add_argument(\"--ddp\", action=\"store_true\", help=\"Use DistributedDataParallel\")\n",
    "    parser.add_argument(\"--accumulation_steps\", type=int, default=8, help=\"Gradient accumulation steps\")\n",
    "    parser.add_argument(\"--grad_clip\", type=float, default=1.0, help=\"Gradient clipping threshold\")\n",
    "    parser.add_argument(\"--warmup_iters\", type=int, default=0, help=\"Number of warmup iterations\")\n",
    "    parser.add_argument(\"--log_interval\", type=int, default=100, help=\"Logging interval\")\n",
    "    parser.add_argument(\"--save_interval\", type=int, default=1000, help=\"Model saving interval\")\n",
    "    parser.add_argument('--local_rank', type=int, default=-1, help='local rank for distributed training')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    lm_config = LMConfig()\n",
    "    max_seq_len = lm_config.max_seq_len\n",
    "    args.save_dir = os.path.join(args.out_dir)\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "    tokens_per_iter = args.batch_size * max_seq_len\n",
    "    torch.manual_seed(1337)\n",
    "    device_type = \"cuda\" if \"cuda\" in args.device else \"cpu\"\n",
    "\n",
    "    args.wandb_run_name = f\"MiniMind-Pretrain-Epoch-{args.epochs}-BatchSize-{args.batch_size}-LearningRate-{args.learning_rate}\"\n",
    "\n",
    "    ctx = nullcontext() if device_type == \"cpu\" else torch.cuda.amp.autocast()\n",
    "\n",
    "    ddp = int(os.environ.get(\"RANK\", -1)) != -1  # is this a ddp run?\n",
    "    ddp_local_rank, DEVICE = 0, \"cuda:0\"\n",
    "    if ddp:\n",
    "        init_distributed_mode()\n",
    "        args.device = torch.device(DEVICE)\n",
    "\n",
    "    if args.use_wandb and (not ddp or ddp_local_rank == 0):\n",
    "        import wandb\n",
    "\n",
    "        wandb.init(project=args.wandb_project, name=args.wandb_run_name)\n",
    "    else:\n",
    "        wandb = None\n",
    "\n",
    "    model, tokenizer = init_model()\n",
    "    df = pd.read_csv(args.data_path)\n",
    "    df = df.sample(frac=1.0)\n",
    "    train_ds = PretrainDataset(df, tokenizer, max_length=max_seq_len)\n",
    "    train_sampler = DistributedSampler(train_ds) if ddp else None\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_workers,\n",
    "        sampler=train_sampler\n",
    "    )\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype in ['float16', 'bfloat16']))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    if False and platform.system() != 'Windows' and float(torch.__version__.split('.')[0]) >= 2:\n",
    "        Logger(\"compiling the model... (takes a ~minute)\")\n",
    "        unoptimized_model = model\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    if ddp:\n",
    "        model._ddp_params_and_buffers_to_ignore = {\"pos_cis\"}\n",
    "        model = DistributedDataParallel(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "    iter_per_epoch = len(train_loader)\n",
    "    for epoch in range(args.epochs):\n",
    "        train_epoch(epoch, wandb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
