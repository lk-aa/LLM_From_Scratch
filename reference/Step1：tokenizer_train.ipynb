{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be813011",
   "metadata": {},
   "source": [
    "# tokenizer 训练详解\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1504d540-4be5-4e7e-9860-fa2c494dcbda",
   "metadata": {},
   "source": [
    "- Step 1.导入必要的库\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b85b0e7-a16c-407c-889b-a15e899bf0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm  \n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55c37baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Using GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29861abc-d6ea-4deb-a437-42601f73cfbc",
   "metadata": {},
   "source": [
    "- Step 2.读取 tokenizer_train.jsonl 文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88e1dc03-f9be-4ede-b8f3-5422afbf25a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好的。现在请你将这个文本中的所有的逗号都替换成空格。 好的，请稍等一下，现在我会将文本中的所有逗号替换为空格。处理后文本为：\"这是一个句子 目的是看看是否可以正确地从这个句子中删除关键词。\"。处理结果如何？\n",
      "帮我回答一道历史题目。清朝时期的八旗共有多少旗人？ 清朝时期八旗旗人总数约为200万人左右，其中正黄旗、正蓝旗、正白旗、正红旗的人数较多，其他旗的人数较少。\n",
      "嗯，谢谢你介绍的做法很详细，但我不喜欢吃鸡蛋，有没有其他菜做法能介绍一下？ 当然，你可以试试酸辣土豆丝这道菜。\n",
      "材料：\n",
      "土豆2个、红椒1个、青椒1个、大葱1根、醋、生抽、盐、鸡精、料酒\n",
      "做法：\n",
      "1.土豆去皮，切成丝；红椒和青椒切成细丝；大葱切段备用。\n",
      "2.热锅凉油，油热后放入土豆丝，煸炒至变软。\n",
      "3.倒入红椒、青椒和大葱段，继续煸炒至熟。\n",
      "4.加入适量的盐、鸡精、料酒和生抽，翻炒均匀。\n",
      "5.最后，加入适量的醋，翻炒均匀即可。\n",
      "小贴士：\n",
      "1. 土豆切丝时，可以放入淡盐水中泡一下，这样可以去除多余的淀粉。\n",
      "2. 煮土豆丝时，不要煮得太久，以免烂糊。\n",
      "3. 加入醋的时候，根据自己的口味多少来进行调节，一般来说，盐与醋的比例为1:1。\n",
      "4. 如果喜欢辣味可以加入一些干辣椒丝。\n",
      "希望你会喜欢这道酸辣土豆丝！\n"
     ]
    }
   ],
   "source": [
    "def read_texts_from_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            yield data['text']\n",
    "\n",
    "# 测试读取数据\n",
    "data_path = '../dataset/tokenizer_train.jsonl'\n",
    "texts = read_texts_from_jsonl(data_path)\n",
    "\n",
    "# 打印前几行文本\n",
    "for i, text in enumerate(texts):\n",
    "    if i < 3:\n",
    "        print(text)\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914305ae-436f-47ba-bd1f-6f3b7bbdce61",
   "metadata": {},
   "source": [
    "- Step 3.初始化分词器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3958bd2a-154c-4d63-ba75-82a36c92b7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词器初始化成功，准备训练。\n"
     ]
    }
   ],
   "source": [
    "# 初始化tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "# 定义特殊token\n",
    "special_tokens = [\"<unk>\", \"<s>\", \"</s>\"]\n",
    "\n",
    "# 设置训练器并添加特殊token\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=6400,\n",
    "    special_tokens=special_tokens,  # 确保这三个token被包含\n",
    "    show_progress=True,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n",
    ")\n",
    "\n",
    "print(\"分词器初始化成功，准备训练。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6902f123-7c3e-4d73-be1a-0178ff896f33",
   "metadata": {},
   "source": [
    "- Step 4.训练分词器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bd14d6e-a14f-47dd-aaa8-7b6e8e60daf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词器训练完成！\n"
     ]
    }
   ],
   "source": [
    "# 读取文本数据\n",
    "texts = read_texts_from_jsonl(data_path)\n",
    "\n",
    "# 训练tokenizer\n",
    "tokenizer.train_from_iterator(texts, trainer=trainer)\n",
    "\n",
    "print(\"分词器训练完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d59390f-092b-46c6-9239-0325a777b234",
   "metadata": {},
   "source": [
    "- Step 5.保存分词器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6b40976-3741-4b09-9147-08e031e31de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer 保存成功！\n"
     ]
    }
   ],
   "source": [
    "# 设置解码器\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# 保存tokenizer\n",
    "tokenizer_dir = \"../model/mateconv_tokenizer\"\n",
    "os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "tokenizer.save(os.path.join(tokenizer_dir, \"tokenizer.json\"))\n",
    "tokenizer.model.save(\"../model/mateconv_tokenizer\")\n",
    "\n",
    "# 手动创建配置文件\n",
    "config = {\n",
    "    \"add_bos_token\": False,\n",
    "    \"add_eos_token\": False,\n",
    "    \"add_prefix_space\": True,\n",
    "    \"added_tokens_decoder\": {\n",
    "        \"0\": {\n",
    "            \"content\": \"<unk>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "            },\n",
    "        \"1\": {\n",
    "            \"content\": \"<s>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "            },\n",
    "        \"2\": {\n",
    "            \"content\": \"</s>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "            }\n",
    "    },\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"clean_up_tokenization_spaces\": False,\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"legacy\": True,\n",
    "    \"model_max_length\": 1000000000000000019884624838656,\n",
    "    \"pad_token\": None,\n",
    "    \"sp_model_kwargs\": {},\n",
    "    \"spaces_between_special_tokens\": False,\n",
    "    \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"use_default_system_prompt\": False,\n",
    "    \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<s>user\\\\n' + content + '</s>\\\\n<s>assistant\\\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '</s>' + '\\\\n' }}{% endif %}{% endfor %}\"\n",
    "}\n",
    "\n",
    "# 保存配置文件\n",
    "with open(os.path.join(tokenizer_dir, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as config_file:\n",
    "    json.dump(config, config_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Tokenizer 保存成功！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ba1478-98f4-4433-b157-2b5f09b86b91",
   "metadata": {},
   "source": [
    "- Step 6.评估分词器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "555b238b-d349-4f03-a33c-50edd757eb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[608, 1589, 4835, 269, 4833, 954, 4725, 270, 1170, 345, 4584, 5204, 1273, 648, 2207, 1, 320, 275, 201, 345, 1390, 258, 3852, 1081, 269, 2, 201, 1, 1078, 538, 501, 201, 22, 23, 24, 2, 201, 1, 320, 275, 201, 22, 23, 24, 2, 201, 1, 1078, 538, 501, 201, 25, 26, 27, 2, 201]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 加载预训练的tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../model/mateconv_tokenizer\")\n",
    "\n",
    "# 测试一段对话\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个优秀的聊天机器人，总是给我正确的回应！\"},\n",
    "    {\"role\": \"user\", \"content\": '是椭圆形的'},\n",
    "    {\"role\": \"assistant\", \"content\": '456'},\n",
    "    {\"role\": \"user\", \"content\": '456'},\n",
    "    {\"role\": \"assistant\", \"content\": '789'}\n",
    "]\n",
    "\n",
    "# 使用模板进行文本处理\n",
    "new_prompt = tokenizer.apply_chat_template(messages, tokenize=True)\n",
    "print(new_prompt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
